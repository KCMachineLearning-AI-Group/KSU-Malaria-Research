{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Regression\n",
    "#### Remove highly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, r2_score\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from model_validation import ModelValidation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in data and one-hot-encode int dtype variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"../data/Series3_6.15.17_padel.csv\", index_col=0)\n",
    "# Eliminate features without variance\n",
    "df = df.loc[:, (df.std() > 0).values]\n",
    "# Seperate Series 3 test when IC50 is null\n",
    "test_index = df.IC50.isnull()\n",
    "test_df = df.loc[test_index]\n",
    "df = df.loc[~test_index]\n",
    "# Remove columns with missing data\n",
    "df = df.dropna(axis=1)\n",
    "# Transform discrete with one-hot-encoding\n",
    "int_cols = df.columns[df.dtypes == 'int64']\n",
    "float_cols = df.columns[df.dtypes == 'float64']\n",
    "one_hot_df = pd.get_dummies(df[int_cols].astype('O'))\n",
    "df = pd.merge(df[float_cols], one_hot_df, left_index=True, right_index=True)\n",
    "# Split x, y\n",
    "y_data = df.pop(\"IC50\")\n",
    "x_data = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure no (+/-) inf or nan due to improper transformation\n",
    "x_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "assert not sum(x_data.isna().sum()), \"Unexpected nulls found\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform non-linear transformations to Series 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform feature engineering on float columns\n",
    "for feat in x_data.columns[x_data.dtypes == 'float64']:\n",
    "    feature_df = x_data.loc[:, feat]\n",
    "    if feature_df.min() > 0:  # Avoid 0 or negative\n",
    "        x_data.loc[:, feat + \"_log\"] = feature_df.apply(np.log)  # log\n",
    "        x_data.loc[:, feat + \"_log2\"] = feature_df.apply(np.log2)  # log2\n",
    "        x_data.loc[:, feat + \"_log10\"] = feature_df.apply(np.log10)  # log10\n",
    "        x_data.loc[:, feat + \"_cubert\"] = feature_df.apply(\n",
    "            lambda x: np.power(x, 1 / 3))  # cube root\n",
    "        x_data.loc[:, feat + \"_sqrt\"] = feature_df.apply(np.sqrt)  # square root\n",
    "    # Avoid extremely large values, keep around 1M max\n",
    "    if feature_df.max() < 13:\n",
    "        x_data.loc[:, feat + \"_exp\"] = feature_df.apply(np.exp)  # exp\n",
    "    if feature_df.max() < 20:\n",
    "        x_data.loc[:, feat + \"_exp2\"] = feature_df.apply(np.exp2)  # exp2\n",
    "    if feature_df.max() < 100:\n",
    "        x_data.loc[:, feat + \"_cube\"] = feature_df.apply(\n",
    "            lambda x: np.power(x, 3))  # cube\n",
    "    if feature_df.max() < 1000:\n",
    "        x_data.loc[:, feat + \"_sq\"] = feature_df.apply(np.square)  # square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group features if correlated > .91 and randomly select 1 from each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove highly correlated features, group by correlation and randomly \n",
    "# select 1 from each group\n",
    "\n",
    "corr_threshold = .91\n",
    "corr_matrix = x_data.corr()\n",
    "corr_matrix.loc[:, :] = np.tril(corr_matrix, k=-1)\n",
    "\n",
    "already_in = set()\n",
    "corr_result = []\n",
    "for col in corr_matrix:\n",
    "    correlated = corr_matrix[col][\n",
    "        np.abs(corr_matrix[col]) > corr_threshold].index.tolist()\n",
    "    if correlated and col not in already_in:\n",
    "        already_in.update(set(correlated))\n",
    "        correlated.append(col)\n",
    "        corr_result.append(correlated)\n",
    "    elif col not in already_in:\n",
    "        already_in.update(set([col]))\n",
    "        corr_result.append([col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_correlated_feats = [corr_feats[0] for corr_feats in corr_result]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "x_scaler = StandardScaler()\n",
    "y_scaler = StandardScaler()\n",
    "\n",
    "x_data.loc[:, :] = x_scaler.fit_transform(x_data)\n",
    "y_data.loc[:] = np.squeeze(y_scaler.fit_transform(y_data.values.reshape(-1, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Grid-search to tune SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark for Support Vector regression\n",
    "model = LinearSVR()\n",
    "params = {\"C\": np.arange(.1, 1.1, .1)}\n",
    "grid = GridSearchCV(model, param_grid=params, \n",
    "                    scoring=make_scorer(r2_score, greater_is_better=True), \n",
    "                    cv=10, n_jobs=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with 3 splits and 10 repeats\n",
      "average r2_score: 0.18796119920413676\n",
      "average rmse: 0.8708130672434643\n",
      "R2 Score: 0.18796119920413676\n"
     ]
    }
   ],
   "source": [
    "grid.fit(x_data.loc[:, non_correlated_feats], y_data)\n",
    "\n",
    "validate = ModelValidation()\n",
    "results = validate.score_regressor(x_data.loc[:, non_correlated_feats], \n",
    "                                   y_data, \n",
    "                                   grid.best_estimator_, \n",
    "                                   pos_split=y_scaler.transform([[2.1]]))\n",
    "\n",
    "print(\"R2 Score: %s\" % np.mean(results[\"r2_score\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
