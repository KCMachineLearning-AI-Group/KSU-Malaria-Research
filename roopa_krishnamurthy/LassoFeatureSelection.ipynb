{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LassoFeatureSelection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, roc_auc_score, log_loss\n",
    "from sklearn.linear_model import SGDRegressor, Lasso\n",
    "from sklearn.preprocessing import StandardScaler, scale\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from pandas import DataFrame, Series\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from numpy import sqrt, mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read in data and one-hot-encode int dtype variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"../data/Series3_6.15.17_padel.csv\", index_col=0)\n",
    "# Eliminate features without variance\n",
    "df = df.loc[:, (df.std() > 0).values]\n",
    "# Seperate Series 3 test when IC50 is null\n",
    "test_index = df.IC50.isnull()\n",
    "test_df = df.loc[test_index]\n",
    "df = df.loc[~test_index]\n",
    "# Remove columns with missing data\n",
    "df = df.dropna(axis=1)\n",
    "# Transform discrete with one-hot-encoding\n",
    "int_cols = df.columns[df.dtypes == 'int64']\n",
    "float_cols = df.columns[df.dtypes == 'float64']\n",
    "one_hot_df = pd.get_dummies(df[int_cols].astype('O'))\n",
    "df = pd.merge(df[float_cols], one_hot_df, left_index=True, right_index=True)\n",
    "# Split x, y\n",
    "y_data = df.pop(\"IC50\")\n",
    "x_data = df.copy()\n",
    "columns = x_data.columns.values\n",
    "# Ensure no (+/-) inf or nan due to improper transformation\n",
    "x_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "assert not sum(x_data.isna().sum()), \"Unexpected nulls found\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perform non-linear transformations to Series 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform feature engineering on float columns\n",
    "for feat in x_data.columns[x_data.dtypes == 'float64']:\n",
    "    feature_df = x_data.loc[:, feat]\n",
    "    if feature_df.min() > 0:  # Avoid 0 or negative\n",
    "        x_data.loc[:, feat + \"_log\"] = feature_df.apply(np.log)  # log\n",
    "        x_data.loc[:, feat + \"_log2\"] = feature_df.apply(np.log2)  # log2\n",
    "        x_data.loc[:, feat + \"_log10\"] = feature_df.apply(np.log10)  # log10\n",
    "        x_data.loc[:, feat + \"_cubert\"] = feature_df.apply(\n",
    "            lambda x: np.power(x, 1 / 3))  # cube root\n",
    "        x_data.loc[:, feat + \"_sqrt\"] = feature_df.apply(np.sqrt)  # square root\n",
    "    # Avoid extremely large values, keep around 1M max\n",
    "    if feature_df.max() < 13:\n",
    "        x_data.loc[:, feat + \"_exp\"] = feature_df.apply(np.exp)  # exp\n",
    "    if feature_df.max() < 20:\n",
    "        x_data.loc[:, feat + \"_exp2\"] = feature_df.apply(np.exp2)  # exp2\n",
    "    if feature_df.max() < 100:\n",
    "        x_data.loc[:, feat + \"_cube\"] = feature_df.apply(\n",
    "            lambda x: np.power(x, 3))  # cube\n",
    "    if feature_df.max() < 1000:\n",
    "        x_data.loc[:, feat + \"_sq\"] = feature_df.apply(np.square)  # square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaler = StandardScaler()\n",
    "y_scaler = StandardScaler()\n",
    "x_data = x_scaler.fit_transform(x_data)\n",
    "x_data =pd.DataFrame(x_data)\n",
    "y_data.loc[:] = np.squeeze(y_scaler.fit_transform(y_data.values.reshape(-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.90255391  0.69200132  0.1175471  ... -0.21081851 -0.26111648\n",
      "  -0.21081851]\n",
      " [ 0.77015275  2.56513217 -1.22197365 ...  4.74341649 -0.26111648\n",
      "  -0.21081851]\n",
      " [ 0.50327655 -0.57088223 -1.69089872 ... -0.21081851 -0.26111648\n",
      "  -0.21081851]\n",
      " ...\n",
      " [-0.252076   -0.31534062 -0.05148966 ... -0.21081851 -0.26111648\n",
      "  -0.21081851]\n",
      " [-0.74967585 -0.1591759  -0.30767016 ... -0.21081851 -0.26111648\n",
      "  -0.21081851]\n",
      " [-0.84699787 -0.69564664 -0.92627181 ... -0.21081851 -0.26111648\n",
      "  -0.21081851]]\n"
     ]
    }
   ],
   "source": [
    "lasso = Lasso(alpha=0.15)\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "select_from_lasso = SelectFromModel(lasso)\n",
    "select_from_lasso.fit(x_data, y_data)\n",
    "n_features = select_from_lasso.transform(x_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['apol' 0.0]\n",
      " ['ATS0m' -0.0]\n",
      " ['ATS1m' 0.0]\n",
      " ...\n",
      " ['Zagreb_156' -0.0]\n",
      " ['Zagreb_180' -0.0]\n",
      " ['Zagreb_182' -0.0]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Series' objects are mutable, thus they cannot be hashed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-148-22a3ba3d3f20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mselected_features_lasso\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#selected_features_lasso = set([x[0] for x in coefficients_lasso.values if x[1] != 0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mselected_features_lasso\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'yes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#x_data = x_data.loc[:, selected_features_lasso[0]].copy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m__contains__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1692\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mAppender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_index_shared_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'__contains__'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0m_index_doc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1694\u001b[0;31m         \u001b[0mhash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1695\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1696\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1043\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m         raise TypeError('{0!r} objects are mutable, thus they cannot be'\n\u001b[0;32m-> 1045\u001b[0;31m                         ' hashed'.format(self.__class__.__name__))\n\u001b[0m\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Series' objects are mutable, thus they cannot be hashed"
     ]
    }
   ],
   "source": [
    "coefficients_lasso = pd.concat([pd.Series(columns),pd.Series(np.transpose(select_features_model.coef_))], axis = 1)\n",
    "print(coefficients_lasso.values)\n",
    "selected_features_lasso = []\n",
    "for index, row in coefficients_lasso.iterrows():\n",
    "    if row[1]!=0:\n",
    "        selected_features_lasso.append(row)     \n",
    "#selected_features_lasso = set([x[0] for x in coefficients_lasso.values if x[1] != 0])  \n",
    "if selected_features_lasso[0] in x_data.columns:\n",
    "    print('yes')\n",
    "#x_data = x_data.loc[:, selected_features_lasso[0]].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def variance_scorer(x, y):\n",
    "    \"\"\"    \n",
    "    Get the variance for each column of X.\n",
    "    \n",
    "    Because principal components have decreasing variance\n",
    "    (i.e. PC4 has less variance than PC3 which has less variance\n",
    "    than PC2 etc.), we can use this function in SelectKBest to select\n",
    "    only the top X number of principal components.\n",
    "    \n",
    "    \"\"\"\n",
    "    scores = [np.var(column) for column in x.T]\n",
    "    return scores, np.array([np.NaN]*len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('select', SelectKBest(k=5, score_func=<function variance_scorer at 0x7feb3d71b158>)), ('regress', SGDRegressor(alpha=0.15, average=False, epsilon=0.1, eta0=0.01,\n",
       "       fit_intercept=True, l1_ratio=0.3, learning_rate='invscaling',\n",
       "       loss='huber', max_iter=10, n_iter=None, penalty='l1', power_t=0.25,\n",
       "       random_state=0, shuffle=True, tol=None, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Pipeline(steps=[\n",
    "            ('feature_selection', SelectFromModel(Lasso(alpha=0.15))),\n",
    "            ('regress', SGDRegressor(random_state=0))\n",
    "        ])\n",
    "\n",
    "model.set_params(regress__loss='huber', regress__penalty='l1', \n",
    "                 regress__alpha=0.15, regress__l1_ratio=0.30, regress__max_iter=10)\n",
    "model.fit(x_data,y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_regressor(x_data, y_data, model, add_train_data=None, verbose=1, pos_split=10):\n",
    "    \"\"\"\n",
    "    Model validation for producing comparable model evaluation. Uses Stratified K-Fold LOOCV adapted\n",
    "    for regression with the positive equivalent <10 IC50, producing 5 folds.\n",
    "    :param x_data: Pandas DataFrame object, Series 3 with 47 examples\n",
    "    :param y_data: Pandas DataFrame or Series object, float datatype, target variables for Series 3\n",
    "    :param model: must have fit and predict method, use sklearn or wrapper\n",
    "    :param add_train_data: Additional data to be evenly spread across train splits\n",
    "    :param verbose: If 0, return dictionary only, if 1 printed results\n",
    "    :param pos_split: cutoff for positive class in StratifiedKFold (y<pos_split)\n",
    "    :return: dictionary\n",
    "    \"\"\"\n",
    "    assert isinstance(x_data, DataFrame), \"x_data must be a pandas DataFrame\"\n",
    "    assert isinstance(y_data, DataFrame) or isinstance(y_data, Series), \"y_data must be pandas DataFrame or Series\"\n",
    "    assert y_data.dtypes == \"float\", \"Expected y_data to be float dtype and received {}\".format(y_data.dtypes)\n",
    "\n",
    "    if add_train_data is not None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # create logging dictionary to track scores\n",
    "    scoring_dict = {\"r2_score\": [], \"rmse\": []}\n",
    "    # create y_class series for Stratified K-Fold split at pos_split\n",
    "    y_class = Series(data=[int(y < pos_split) for y in y_data])\n",
    "    # num_splits count number of positive examples\n",
    "    num_splits = sum(y_class.values)\n",
    "    scoring_dict[\"num_splits\"] = num_splits\n",
    "    # create splits using stratified kfold\n",
    "    rskf = RepeatedStratifiedKFold(n_splits=num_splits, n_repeats=10, random_state=36851234)\n",
    "    # loop through splits\n",
    "    for train, test in rskf.split(x_data, y_class):\n",
    "        x_train, x_test = x_data.iloc[train, :], x_data.iloc[test, :]\n",
    "        y_train, y_test = y_data.iloc[train], y_data.iloc[test]\n",
    "        # train model, test model with all scoring parameters\n",
    "        model.fit(x_train, y_train)\n",
    "        y_ = model.predict(x_test)\n",
    "        # append scores to logging dictionary\n",
    "        scoring_dict[\"r2_score\"].append(r2_score(y_test, y_))\n",
    "        scoring_dict[\"rmse\"].append(sqrt(mean_squared_error(y_test, y_)))\n",
    "    if verbose == 1:\n",
    "        # Print contents of dictionary except confusion matrix\n",
    "        print(\"with {} splits and {} repeats\".format(num_splits, 10))\n",
    "        for metric in scoring_dict:\n",
    "            if metric == \"num_splits\":\n",
    "                continue\n",
    "            else:\n",
    "                print(\"average {}: {}\".format(metric, mean(scoring_dict[metric])))\n",
    "    return scoring_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with 5 splits and 10 repeats\n",
      "average r2_score: -0.20301890116385365\n",
      "average rmse: 29.567772540388663\n"
     ]
    }
   ],
   "source": [
    "results = score_regressor(x_data= x_data_pca, y_data = y_data, model = grid.best_estimator_)\n",
    "                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
